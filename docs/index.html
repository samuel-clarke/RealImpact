<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body {
            font-family: sans-serif;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            text-align: center;
        }
        h1 {
            font-size: 36px;
            margin-bottom: 30px;
            padding-top: 30px;
        }
        h2 {
            font-size: 24px;
            margin-top: 40px;
            margin-bottom: 10px;
        }
        h3 {
            font-size: 22px;
            margin-top: 10px;
            margin-bottom: 10px;
        }
        p {
            font-size: 16px;
            line-height: 1.5;
            margin-bottom: 20px;
            margin-left: 10%;
            margin-right: 10%;
            text-align: justify;
        }
        .author {
            margin-right: 10px;
            margin-left: 10px;
        }
        a {
            text-decoration: none;
            color: navy;
        }
        a:hover {
            color: lightblue;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>RealImpact: A Dataset of Impact Sound Fields for Real Objects</h1>
        <h3>
            <a href="https://samuelpclarke.com/"><span class="author">Samuel Clarke</span></a>
            <a href="https://ai.stanford.edu/~rhgao/"><span class="author">Ruohan Gao</span></a>
            <a href="https://www.linkedin.com/in/mason-wang-3b5288104/"><span class="author">Mason Wang</span></a>
            <a href="https://ccrma.stanford.edu/~mrau/"><span class="author">Mark Rau</span></a>
        </h3>
        <h3>
            <a href="https://www.linkedin.com/in/julia-xu-709167127"><span class="author">Julia Xu</span></a>
            <a href="http://juiwang.com/"><span class="author">Jui-Hsien Wang</span></a>
            <a href="https://jiajunwu.com/"><span class="author">Doug James</span></a>
            <a href="https://graphics.stanford.edu/~djames/"><span class="author">Jiajun Wu</span></a>
        </h3>
        <h2>Abstract</h2>
        <hr width="70%">
        <p>Objects make unique sounds under different perturbations, environment conditions, and poses relative to the listener.
            While prior works have modeled impact sounds and sound propagation in simulation, we lack a standard dataset of impact 
            sound fields of real objects for audio-visual learning and calibration of the sim-to-real gap. 
            We present RealImpact, a large-scale dataset of real object impact sounds recorded under
            controlled conditions. RealImpact contains 150,000 recordings of impact sounds of 50 everyday objects with detailed annotations, 
            including their impact locations, microphone locations, contact force profiles, material labels, and RGBD images. 
            We make preliminary attempts to use our dataset as a reference to current simulation methods for estimating object impact sounds 
            that match the real world. Moreover, we demonstrate the usefulness of our dataset as a testbed for acoustic and audio-visual learning 
            via the evaluation of two benchmark tasks, including listener location classification and visual acoustic matching.</p>
        <!-- Rest of the content goes here -->
    </div>
</body>
</html>

